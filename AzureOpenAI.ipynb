{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index==0.12.26 llama-index-llms-azure-openai==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from openai import AzureOpenAI \n",
    "from azure.identity import defaultAzureCredential, get_bearer_token_provider \n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine = deployment, \n",
    "    model = \"gpt-4o\", #For caching using version 2024-12-17 and above,\n",
    "    api_key = subscription_key, \n",
    "    azure_endpoint = endpoint, \n",
    "    api_version = \"2025-01-01-preview\",\n",
    "    max_token = 800,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95, \n",
    "    frequency_penalty = 0, \n",
    "    presence_penalty = 0, \n",
    "    stop = None, \n",
    "    stream = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Unstructured Loader to load directly into Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antropic's Contextual Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from typing import Callable, Optional, List, Coroutine\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from llama_index.core.llms import (\n",
    "    ChatMessage,\n",
    "    ImageBlock,\n",
    "    TextBlock,\n",
    "    ChatResponse\n",
    ")\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def retry_on_rate_limit(retries: int = 3, backoff_factor: int = 2):\n",
    "    \"\"\"\n",
    "    Decorator to retry an async function when rate limit errors occur.\n",
    "\n",
    "    Args:\n",
    "        retries (int): Maximum number of retry attempts.\n",
    "        backoff_factor (int): Exponential backoff factor (e.g., 2 means 2^attempt seconds).\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable[..., Coroutine]) -> Callable[..., Coroutine]:\n",
    "        @wraps(func)\n",
    "        async def wrapper(*args, **kwargs) -> Optional[ChatResponse]:\n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    return await func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    error_message = str(e).lower()\n",
    "                    if any(msg in error_message for msg in [\"rate limit\", \"too many requests\", \"429\"]):\n",
    "                        logger.warning(f\"Rate limit reached. Retrying {attempt + 1}/{retries}...\")\n",
    "                        await asyncio.sleep(backoff_factor ** attempt)  # Exponential backoff\n",
    "                    else:\n",
    "                        logger.error(f\"Unexpected error: {e}\")\n",
    "                        break\n",
    "            return None  # Return None if all retries fail\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry_on_rate_limit()\n",
    "async def _generate_response(messages: List[ChatMessage]) -> Optional[ChatResponse]:\n",
    "    \"\"\"\n",
    "    Generates a response for a user input.\n",
    "\n",
    "    Args:\n",
    "        messages (List[ChatMessage]): A list of chat messages forming the prompt.\n",
    "\n",
    "    Returns:\n",
    "        Optional[ChatResponse]: The AI-generated response or None in case of an error.\n",
    "    \"\"\"\n",
    "    return await llm.achat(messages)\n",
    "\n",
    "async def generate_chunk_context(document_text: str, chunk_text: str) -> Optional[ChatResponse]:\n",
    "    \"\"\"\n",
    "    Generates a succinct context that situates a chunk within the overall document.\n",
    "\n",
    "    Args:\n",
    "        document_text (str): The full document text.\n",
    "        chunk_text (str): A chunk of text from the document.\n",
    "\n",
    "    Returns:\n",
    "        Optional[ChatResponse]: A short context to situate the chunk within the document.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            blocks=[TextBlock(text=document_text)],  # Check if there's a cache hit\n",
    "        ),\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=[\n",
    "                TextBlock(text=\"Here is the chunk we want to situate within the whole document.\"),\n",
    "                TextBlock(text=chunk_text),\n",
    "                TextBlock(\n",
    "                    text=\"Please provide a short, succinct context situating this chunk within the overall document \"\n",
    "                    \"to improve search retrieval. Answer only with the succinct context and nothing else.\"\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    return await _generate_response(messages)\n",
    "\n",
    "async def situate_image_within_document(document_text: str, image_base64: str) -> Optional[ChatResponse]:\n",
    "    \"\"\"\n",
    "    Generates a succinct description situating an image within a document.\n",
    "\n",
    "    Args:\n",
    "        document_text (str): The text of the document or a relevant excerpt.\n",
    "        image_base64 (str): A single image in base64 format.\n",
    "\n",
    "    Returns:\n",
    "        Optional[ChatResponse]: A description contextualizing the image within the document.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            blocks=[TextBlock(text=document_text)],  # Check if there's a cache hit\n",
    "        ),\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=[\n",
    "                TextBlock(text=\"Here is the image we want to describe within the context of the document.\"),\n",
    "                ImageBlock(image=image_base64),\n",
    "                TextBlock(\n",
    "                    text=\"Based on the surrounding text in the document, please provide a short, succinct description \"\n",
    "                    \"of this image. The description should situate the image within the document, helping improve \"\n",
    "                    \"search retrieval and relevance. Answer only with the succinct description and nothing else.\"\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    return await _generate_response(messages)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
